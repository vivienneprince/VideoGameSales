---
title: "VideoGameSales"
author: "amanda, ben, and viv"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    toc_float: yes
---

```{r, eval=TRUE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(ggplot2)
library(plotly)
library(boot)
library(ggpubr)
project.dir <- getwd()
dataset.dir <- 'data'
outputs.dir <- 'output'
```

# 0. Data Import and Cleaning {.tabset}

## Data Import
```{r}
videogames.df <- read.csv(file.path(project.dir, dataset.dir, 'vgsales-12-4-2019.csv'))
colnames(videogames.df)
```

## Data cleaning
```{r}
# Since the data was collected in April of 2019, we are excluding games with year = 2019 since it does not give a comprehensive picture of all the sales during 2019. 
videogames.clean <- videogames.df %>% filter(Year < 2019)
# E was originally called KA for ESRB ratings, so we are going to make all the KA ratings E
videogames.clean <- videogames.clean %>% mutate(ESRB_Rating = replace(ESRB_Rating, ESRB_Rating=='KA', 'E'))
# Make give the ESRB rating levels for easier graphing/ data manipulation
unique(videogames.clean$ESRB_Rating)
videogames.clean$ESRB_Rating <- factor(videogames.clean$ESRB_Rating,levels = c('','RP','E', 'EC', 'E10','T','M','AO'))
```

## Data reshaping
We want to compare sales across different regions, so it would be convenient to have one column 'region' and then a corresponding column for sales in USD (millions).
```{r}
vs_byregion <- videogames.clean %>% gather(Region, Sales, Global_Sales:Other_Sales, na.rm = T)
```




# 1. Descriptive analysis {.tabset}

Conduct some descriptive analysis on the data, figuring out:
* distributions of variables,
* variables that appear to be strongly related with each other (using appropriate methods to quantify the relationships based on whether variables are numerical or categorical).

## Distributions {.tabset}

### Sales
From the boxplot we can see that we have 2 extreme outliers. After investigating, it looks like two outliers are GTA V (ps3 and ps4)
```{r}
boxplot(videogames.clean$Global_Sales, xlab = 'Global Sales (millions of USD)')
videogames.clean[which(videogames.clean$Global_Sales > 17), ]
hist(videogames.clean$Global_Sales,
     xlab = 'Global Sales (millions of USD)',
     xlim = c(0, .5),
     breaks = 2000)

videogames.clean %>% arrange(desc(Global_Sales)) %>% select(Name, Platform, Genre, Global_Sales, Year)
```

### Platform 
ggplotly(
videogames.clean %>% 
  count(Platform, sort = TRUE) %>% 
  ggplot(aes(x = reorder(Platform, -n), y = n)) +
  geom_bar(stat = "identity",position = position_dodge(width=0)) +
  theme(axis.text.x=element_text(angle=90,hjust=1, vjust = 0.5))
)


### ESRB Rating 
videogames.clean %>% ggplot(aes(x = ESRB_Rating)) +
  geom_bar()


### Genre 
```{r}
videogames.clean %>% 
  count(Genre, sort = TRUE) %>% 
  ggplot(aes(x = reorder(Genre, -n), y = n)) +
  theme_minimal() +
  geom_bar(stat = "identity") +
  theme(axis.text.x=element_text(angle=45,hjust=1))


```


### Scoring
Here we looked at distribution of User Scores and Critic Scores as well as the average Critic and User Score over time.
videogames.clean %>% ggplot() +
  geom_histogram(binwidth = 0.5,aes(x = Critic_Score, fill = 'pink') ) +
  geom_histogram(binwidth = 0.5,aes(x = User_Score, fill = 'blue') ) 

### Publisher
We have a ton of publishers

### Year
```{r}
ggplotly(
videogames.clean %>% ggplot(aes(x = Year)) +
  theme_minimal() +
  geom_bar()
)
```

## Notable Correlations {.tabset}

### Sales by year 
```{r}
vs_sales.byregion.byyear <- vs_byregion %>% 
  group_by(Year, Region)  %>% 
  summarize(SSales = sum(Sales)) 
vs_sales.byregion.byyear$MSales <- vs_byregion %>% 
  group_by(Year, Region)  %>% 
  summarize(means = mean(Sales)) %>%
  pull(means)

summary.data <- videogames.clean %>%
  group_by(Year) %>%
  summarise(SSales = sum(Global_Sales), 
            MSales = mean(Global_Sales),
            Critic = mean(Critic_Score))



ggplotly(
vs_sales.byregion.byyear %>% ggplot(aes(x=Year))+
  theme_minimal() +
  geom_line(aes(y= SSales, color = Region))+
  geom_line(linetype = "dotted", aes(y= MSales*100, color = Region))+
  geom_bar()
)
```

### Sales per platform
### Sales per rating
### Sales per genre 

### ESRB Rating per year

### Rating by year
```{r}
plot <- videogames.clean %>% group_by(Year) %>% summarise(
  User_Score = mean(User_Score, na.rm = T),
  Critic_Score = mean(Critic_Score, na.rm = T), 
  Vgchartzscore = mean(Vgchartzscore, na.rm = T)) %>% 
  filter(Year >= 1989) %>%
  mutate(User_Score2 = case_when(Year >= 1996 ~ User_Score,
                           TRUE ~ NaN)) %>%
  gather(ScoreType, Score, c(User_Score,Critic_Score,Vgchartzscore), na.rm = T) %>%
  
  
  ggplot(aes(x = Year)) + # TODO : Make look better
  theme_minimal()+
  geom_line(aes(y = Score, color = ScoreType)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  scale_x_continuous('Year', labels = 2000:2018, breaks = 2000:2018) +
  xlab('Year') +
  xlim(2000, 2018) +
  ylim(0, 10)


  ggarrange(plot, 
          labels = c(""),
          ncol = 1, nrow = 2)

```

# rm GTA V outliers
```{r}
videogames.clean %>% filter(Global_Sales < 17 & !is.na(Global_Sales)) %>% arrange(desc(Global_Sales))
```


# 2. One sample stats inference for response variable of interest (sales) 
	‚ÅÉ	construct CI for population mean value for sales

Total sample mean and CI  
```{r}
confidence <- 0.95
n <- length(videogames.clean$Global_Sales)
mu.hat.all <- mean(videogames.clean$Global_Sales, na.rm = T)
sd.hat <- sd(videogames.clean$Global_Sales)

se <- sd.hat/sqrt(n)
alpha <- 1-confidence

CI <- c(mu.hat.all - qt(1-alpha/2, n-1)*se,
        mu.hat.all + qt(1-alpha/2, n-1)*se)

mu.hat.all
CI
```    

One sample inference and CI (2008)  
The test result is statistically significant with a p value of 0.1805, and 95% confidence interval of [0.3095337 0.3778467]. This means that on average, we are 95% confident that 
```{r}
# using all sample mean as population mean:
mu0 <- mu.hat.all
samp2008 <- videogames.clean %>% filter(Year == 2008 & !is.na(Global_Sales))  %>% pull(Global_Sales)
mu.hat <- mean(samp2008)
t.test(samp2008, mu = mu0)
```

Check assumptions:
* The sample is not randomized (vgchartz's game database does not include all games and would have a bias towards including games that are available in english)
* The population sales distribution is not normal at all (extreme right skew)
* The dataset had two extreme outliers identified via boxplot, and removed.

Although the t-distribution CI is robust against non-normal populations, it is highly sensitive to violations of the random sampling assumption. Since our dataset would be missing a disproportionate amount of non-western games, and older games. So we likely have an undercoverage issue using the t-distribution method if we consider our population to be all video games that ever existed worldwide. But if we consider our population to be ???????????? then the CI we have is trustworthy

We can also use bootstrap to estimate the 95% CI for the mean of video game sales. I would not expect this result to be significantly different, or better than the t-distribution method because bootstrapping is also sensitive to non-random sampling, because the assumption is that our sample is a good representation of the population we are interested in.

```{r}
```


# 3. Construct confidence interval for proportion of video games in a certain genre (Action) (conduct hypothesis test against pre-specified constant)
Here we conducted a t test with 95% confidence intervals looking at sales for the Action Genre. We ignored any NA values and made sure to exclude any extreme outliers such as GTA V. H Null is Action Video Games mean sales = 0.367 and Alternative Hypothesis being Action Video Games mean sales != .367. We can see that the mean video game sales does not fall into our 95% confidence intervals for Action game sales. Thus, we reject the null hypothesis.

```{r}

#Ignored all NA values

actions <- videogames.clean %>% filter(Genre == 'Action')

actionconf <- t.test(actions$Global_Sales, mu=0.365, conf.level = 0.95)

actionconf

# 0.3693669 0.4373343

#Null Hypothesis would be that mean video game sales (.365) falls between .369 and .437

avgsales<-mean(videogames.clean$Global_Sales,na.rm=T)
avgsales

#0.365

#p-value = 0.02699

#Mean does not fall into 95% confidence interval so we reject Null Hypothesis.

#Cohen.d(t.test, data= videogames.clean)

# Cohen's effect size
abs(mean(actions$Global_Sales) - mean(videogames.clean$Global_Sales)) / sqrt((sd(actions$Global_Sales)^2 + (sd(videogames.clean$Global_Sales)^2)) / 2)

#0.04389653


```


# 4. Our two-sample test variables will be looking at the differences in sales between two genre types: sports and shooters. We choose these genres as they are most frequent and are very popular with similar gaming crowds ie. FIFA vs. COD essentially.

Assumptions for Two-Sided Significance Test for Comparing Two Population Means: 
1) A quantitative response variable for two groups - in this case is our sales  which is quantitative.
2) Independent Random Samples - our data isn't random 
3) Approx. Normal Population for each group - Not true, the sales of video games has a major right skew as most of the video games sold do not sell very well. The majority of games sell less than $ 1 million US Dollars. 

```{r}
videogames.clean %>% 
  filter(Genre == "Sports") %>%
  ggplot(aes(x=Global_Sales)) +
  geom_histogram()
videogames.clean %>% 
  filter(Genre == "Shooter") %>%
  ggplot(aes(x=Global_Sales)) +
  geom_histogram()
```

```{r}
sports <- videogames.clean %>%
  filter(Genre=="Sports")
shooter <- videogames.clean %>%
  filter(Genre=="Shooter")
```

```{r}
boxplot(sports$Global_Sales, main = "Sports")
boxplot(shooter$Global_Sales, main = "Shooter")
```


```{r}
arrange(sports, desc(Global_Sales))
arrange(shooter, desc(Global_Sales))
```
```{r}
sports.rm <- sports %>%
  filter(Global_Sales < 10)
arrange(sports.rm, desc(Global_Sales))

shooter.rm <- shooter %>%
  filter(Global_Sales < 10)
arrange(shooter.rm, desc(Global_Sales))
```
```{r}
t.test(sports.rm$Global_Sales, shooter.rm$Global_Sales)
```
```{r}
t.test(sports$Global_Sales, shooter$Global_Sales)
```
```{r}
cohend <- abs(mean(shooter.rm$Global_Sales) - mean(sports.rm$Global_Sales)) / sqrt((sd(shooter.rm$Global_Sales)^2 + (sd(sports.rm$Global_Sales)^2)) / 2)
cohend
```
```{r}
cohend <- abs(mean(shooter$Global_Sales) - mean(sports$Global_Sales)) / sqrt((sd(shooter$Global_Sales)^2 + (sd(sports$Global_Sales)^2)) / 2)
cohend
```


# 5. We will (c) construct a confidence interval for population correlation between critic score and sales using bootstrap.

```{r}

# remove NAs from critic scores and sales
vc5 <- videogames.clean2 %>% 
  filter(!is.na(Critic_Score) & ! is.na(Global_Sales)) %>%
  select(Critic_Score, Global_Sales)

## Using boot() function for ST ERR/DISTRIBUTION of:
##  - CORRELATION.

fullsamp.corr <- cor(vc5$Critic_Score, vc5$Global_Sales)  # Full-sample correlation. 0.30

vc5$Critic_Score[which(is.numeric(class(vc5$Critic_Score)))]

r <- function(x, i) {cor(x[i,1], x[i,2])} # corr of columns 1 & 2

library(boot) #for boot function
res <- boot(data = vc5, statistic = r, R = 10000)
res

# Distribution of sample correlation.
hist(res$t, breaks=20)






#####################
### BOOTSTRAP CONFIDENCE INTERVALS
#####################

### For CORRELATION example:

### 95% Confidence interval:

## PROPER way, via 1) accounting for bias; 2) using estimated quantiles.

# To guarantee that our bootstrap distribution is "centered at population parameter",
# we need to make sure it's mean = population parameter.
mean(res$t)                          # That's the bootstrap distribution mean
#cor(law$LSAT,law$GPA)   # Full sample correlations 
                          # acts as our "best guess" at population correlation.
                          # We need to cover it with a 95% probability area, 
                          # !!! centered at it. !!!

#mean(res$t) - cor(law$LSAT,law$GPA)  # That's the BIAS of bootstrap = {estimate - "population parameter"}

bias <- mean(res$t) - fullsamp.corr # = (estimate - parameter)
unbiased.est <- res$t - bias         # "De-biasing" the bootstrap distribution
mean(unbiased.est)                   # Now our bootstrap distribution 
                                     # is centered at "population parameter".

hist(unbiased.est, breaks=20)

# Now we can safely take the quantiles:
c(quantile(unbiased.est, 0.025), 
  quantile(unbiased.est, 0.975))




dat <- data.frame(unbiased.est)
dat <- dat %>% mutate(status = case_when(unbiased.est > 0.2790296 &
                                           unbiased.est < 0.3188720 ~ "in",
                                         TRUE ~ "out"))


dat %>% ggplot() +
  theme_minimal() +
  geom_histogram(color = 'white',
                 bins = 40,
                 aes(unbiased.est,
                     fill = status)) +
  xlab("sample correlation mean") +
  ylab("") +
  geom_point(aes(x = 0.2989303, y =0), shape = "I", size = 4) +
  geom_vline(xintercept = 0.2790296, size = 0.3) +
  geom_vline(xintercept = 0.3188720, size = 0.3)


exp.model <-lm(Global_Sales ~ exp(Critic_Score), vc5)

vc5 %>% ggplot(aes(x = Critic_Score, y = Global_Sales)) +
  theme_minimal() +
  geom_point(color = "#82c4bb")+
  ylab("Sales (mil USD)")+
  xlab("Critic Score (0-10)")+
  ggtitle("Video Game Sales vs Critic Score")+
  geom_smooth(method="lm", color="#eb5600", formula= (y ~ exp(x)), se=FALSE, size = .7) 
  #geom_smooth(method = "lm", color = '#eb5600', se = FALSE, size = .7, linetype = 2)
```

```{r}

# remove NAs from critic scores and sales
vc5 <- videogames.clean2 %>% 
  filter(!is.na(User_Score) & ! is.na(Global_Sales)) %>%
  select(User_Score, Global_Sales)

## Using boot() function for ST ERR/DISTRIBUTION of:
##  - CORRELATION.

fullsamp.corr <- cor(vc5$User_Score, vc5$Global_Sales)  # Full-sample correlation. 0.30

r <- function(x, i) {cor(x[i,1], x[i,2])} # corr of columns 1 & 2

library(boot) #for boot function
res <- boot(data = vc5, statistic = r, R = 10000)
res

# Distribution of sample correlation.
hist(res$t, breaks=20)





#####################
### BOOTSTRAP CONFIDENCE INTERVALS
#####################

### For CORRELATION example:

### 95% Confidence interval:

## PROPER way, via 1) accounting for bias; 2) using estimated quantiles.

# To guarantee that our bootstrap distribution is "centered at population parameter",
# we need to make sure it's mean = population parameter.
mean(res$t)                          # That's the bootstrap distribution mean
#cor(law$LSAT,law$GPA)   # Full sample correlations 
                          # acts as our "best guess" at population correlation.
                          # We need to cover it with a 95% probability area, 
                          # !!! centered at it. !!!

#mean(res$t) - cor(law$LSAT,law$GPA)  # That's the BIAS of bootstrap = {estimate - "population parameter"}

bias <- mean(res$t) - fullsamp.corr # = (estimate - parameter)
unbiased.est <- res$t - bias         # "De-biasing" the bootstrap distribution
mean(unbiased.est)                   # Now our bootstrap distribution 
                                     # is centered at "population parameter".

hist(unbiased.est, breaks=20)

# Now we can safely take the quantiles:
c(quantile(unbiased.est, 0.025), 
  quantile(unbiased.est, 0.975))




dat <- data.frame(unbiased.est)
dat <- dat %>% mutate(status = case_when(unbiased.est > 0.1353552 &
                                           unbiased.est < 0.3391475 ~ "in",
                                         TRUE ~ "out"))


dat %>% ggplot() +
  theme_minimal() +
  geom_histogram(color = 'white',
                 bins = 40,
                 aes(unbiased.est,
                     fill = status)) +
  xlab("sample correlation mean") +
  ylab("") +
  geom_point(aes(x = 0.2416502, y =0), shape = "I", size = 4) +
  geom_vline(xintercept = 0.1353552, size = 0.3) +
  geom_vline(xintercept = 0.3391475, size = 0.3)


vc5 %>% ggplot(aes(x = User_Score, y = Global_Sales)) +
  theme_minimal() +
  geom_point(color = "#82c4bb")+
  ylab("Sales (mil USD)")+
  xlab("User Rating (0-10)")+
  geom_smooth(method = "lm", color = '#eb5600', se = FALSE, size = .7)
```



```{r}
videogames.clean %>% 
  filter(Name == "Yakuza Zero" | 
           str_detect(Name, "Stardew Valley") | 
           str_detect(Name, "Metal Gear Solid 2: Sons of Liberty")) %>%
  select(Name, Global_Sales, Year) %>%
  group_by(Name) %>%
  summarise(Global_Sales = sum(Global_Sales)) %>%
  arrange(desc(Global_Sales))
```



